{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaaabd9b",
   "metadata": {},
   "source": [
    "# Pdf extraction with unstructured.io\n",
    "\n",
    "Using the [Yolox paper](https://arxiv.org/pdf/2107.08430.pdf) as a guinea pig for extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c86513a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.staging.base import dict_to_elements, elements_to_text, convert_to_dict\n",
    "import polars as pl\n",
    "from polars import col\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cdb7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unstructured\n",
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64f286e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4af1cb",
   "metadata": {},
   "source": [
    "Not even using a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da17f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_df(df: pl.DataFrame, *, n_rows=100, fmt_str_lengths=1000, tbl_width_chars=1000):\n",
    "    with pl.Config(tbl_rows=n_rows, tbl_cols=-1, fmt_str_lengths=fmt_str_lengths, tbl_width_chars=tbl_width_chars):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "418f74d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_yolox = partition_pdf(\"2107.08430.pdf\", strategy=\"hi_res\", hi_res_model_name=\"yolox\", include_page_breaks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d40b4",
   "metadata": {},
   "source": [
    "This took about 1m on my crappy 5 year old laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4638bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fast = partition_pdf(\"2107.08430.pdf\", strategy=\"fast\", include_page_breaks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f7b90",
   "metadata": {},
   "source": [
    "This is speedy, 1.5s, as it says on the tin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10a2c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yolox = pl.DataFrame(convert_to_dict(out_yolox))\n",
    "df_fast = pl.DataFrame(convert_to_dict(out_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "48a7a870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 298)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yolox.height, df_fast.height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e67d9c9",
   "metadata": {},
   "source": [
    "The approaches give quite different numbers of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37b41156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (11, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>type</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;FigureCaption&quot;</td><td>7</td></tr><tr><td>&quot;Footer&quot;</td><td>7</td></tr><tr><td>&quot;Formula&quot;</td><td>1</td></tr><tr><td>&quot;Header&quot;</td><td>1</td></tr><tr><td>&quot;Image&quot;</td><td>4</td></tr><tr><td>&quot;ListItem&quot;</td><td>33</td></tr><tr><td>&quot;NarrativeText&quot;</td><td>49</td></tr><tr><td>&quot;PageBreak&quot;</td><td>6</td></tr><tr><td>&quot;Table&quot;</td><td>6</td></tr><tr><td>&quot;Title&quot;</td><td>16</td></tr><tr><td>&quot;UncategorizedText&quot;</td><td>2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (11, 2)\n",
       "┌───────────────────┬───────┐\n",
       "│ type              ┆ count │\n",
       "│ ---               ┆ ---   │\n",
       "│ str               ┆ u32   │\n",
       "╞═══════════════════╪═══════╡\n",
       "│ FigureCaption     ┆ 7     │\n",
       "│ Footer            ┆ 7     │\n",
       "│ Formula           ┆ 1     │\n",
       "│ Header            ┆ 1     │\n",
       "│ Image             ┆ 4     │\n",
       "│ ListItem          ┆ 33    │\n",
       "│ NarrativeText     ┆ 49    │\n",
       "│ PageBreak         ┆ 6     │\n",
       "│ Table             ┆ 6     │\n",
       "│ Title             ┆ 16    │\n",
       "│ UncategorizedText ┆ 2     │\n",
       "└───────────────────┴───────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_df(df_yolox[\"type\"].value_counts().sort(\"type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc5c0ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>type</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;ListItem&quot;</td><td>12</td></tr><tr><td>&quot;NarrativeText&quot;</td><td>89</td></tr><tr><td>&quot;PageBreak&quot;</td><td>7</td></tr><tr><td>&quot;Title&quot;</td><td>69</td></tr><tr><td>&quot;UncategorizedText&quot;</td><td>121</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌───────────────────┬───────┐\n",
       "│ type              ┆ count │\n",
       "│ ---               ┆ ---   │\n",
       "│ str               ┆ u32   │\n",
       "╞═══════════════════╪═══════╡\n",
       "│ ListItem          ┆ 12    │\n",
       "│ NarrativeText     ┆ 89    │\n",
       "│ PageBreak         ┆ 7     │\n",
       "│ Title             ┆ 69    │\n",
       "│ UncategorizedText ┆ 121   │\n",
       "└───────────────────┴───────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_df(df_fast[\"type\"].value_counts().sort(\"type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8130da03",
   "metadata": {},
   "source": [
    "When you use yolox you get a lot more categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed26d3",
   "metadata": {},
   "source": [
    "## Is the table category accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58143aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Coupled Head Decoupled Head Vanilla YOLO End-to-end YOLO 38.5 34.3 (-4.2) 39.6 38.8 (-0.8)\n",
      "Methods AP (%) Parameters GFLOPs Latency FPS YOLOv3-ultralytics2 44.3 63.00 M 157.3 10.5 ms 95.2 YOLOv3 baseline +decoupled head +strong augmentation +anchor-free +multi positives +SimOTA +NMS free (optional) 38.5 39.6 (+1.1) 42.0 (+2.4) 42.9 (+0.9) 45.0 (+2.1) 47.3 (+2.3) 46.5 (-0.8) 63.00 M 63.86 M 63.86 M 63.72 M 63.72 M 63.72 M 67.27 M 157.3 186.0 186.0 185.3 185.3 185.3 205.1 10.5 ms 11.6 ms 11.6 ms 11.1 ms 11.1 ms 11.1 ms 13.5 ms 95.2 86.2 86.2 90.1 90.1 90.1 74.1\n",
      "Models AP (%) Parameters GFLOPs Latency YOLOv5-S YOLOX-S 36.7 39.6 (+2.9) 7.3 M 9.0 M 17.1 26.8 8.7 ms 9.8 ms YOLOv5-M 44.5 YOLOX-M 46.4 (+1.9) 21.4 M 25.3 M 51.4 73.8 11.1 ms 12.3 ms YOLOv5-L YOLOX-L 48.2 50.0 (+1.8) 47.1 M 54.2 M 115.6 155.6 13.7 ms 14.5 ms YOLOv5-X 50.4 YOLOX-X 51.2 (+0.8) 87.8 M 99.1 M 219.0 281.9 16.0 ms 17.3 ms\n",
      "Models AP (%) Parameters GFLOPs YOLOv4-Tiny [30] PPYOLO-Tiny YOLOX-Tiny 21.7 22.7 32.8 (+10.1) 6.06 M 4.20 M 5.06 M 6.96 - 6.45 NanoDet3 YOLOX-Nano 23.5 25.3 (+1.8) 0.95 M 0.91 M 1.20 1.08\n",
      "Models Scale Jit. Extra Aug. AP (%) YOLOX-Nano [0.5, 1.5] [0.1, 2.0] - MixUp 25.3 24.0 (-1.3) YOLOX-L [0.1, 2.0] [0.1, 2.0] - MixUp 48.6 49.5 (+0.9) [0.1, 2.0] Copypaste [6] 49.4\n",
      "Method Backbone Size FPS AP (%) AP50 AP75 APS APM APL (V100) YOLOv3 + ASFF* [18] Darknet-53 YOLOv3 + ASFF* [18] Darknet-53 608 800 45.5 29.4 42.4 43.9 63.0 64.1 47.4 49.2 25.5 27.0 45.7 46.6 52.3 53.4 EfﬁcientDet-D0 [28] EfﬁcientDet-D1 [28] EfﬁcientDet-D2 [28] EfﬁcientDet-D3 [28] Efﬁcient-B0 Efﬁcient-B1 Efﬁcient-B2 Efﬁcient-B3 512 640 768 896 98.0 74.1 56.5 34.5 33.8 39.6 43.0 45.8 52.2 58.6 62.3 65.0 35.8 42.3 46.2 49.3 12.0 17.9 22.5 26.6 38.3 44.3 47.0 49.4 51.2 56.0 58.4 59.8 PP-YOLOv2 [11] PP-YOLOv2 [11] ResNet50-vd-dcn ResNet101-vd-dcn 640 640 68.9 50.3 49.5 50.3 68.2 69.0 54.4 55.3 30.7 31.6 52.9 53.9 61.2 62.4 YOLOv4 [1] YOLOv4-CSP [30] CSPDarknet-53 Modiﬁed CSP 608 640 62.0 73.0 43.5 47.5 65.7 66.2 47.3 51.7 26.7 28.2 46.7 51.2 53.3 59.8 YOLOv3-ultralytics2 YOLOv5-M [7] YOLOv5-L [7] YOLOv5-X [7] Darknet-53 Modiﬁed CSP v5 Modiﬁed CSP v5 Modiﬁed CSP v5 640 640 640 640 95.2 90.1 73.0 62.5 44.3 44.5 48.2 50.4 64.6 63.1 66.9 68.8 - - - - - - - - - - - - - - - - YOLOX-DarkNet53 YOLOX-M YOLOX-L YOLOX-X Darknet-53 Modiﬁed CSP v5 Modiﬁed CSP v5 Modiﬁed CSP v5 640 640 640 640 90.1 81.3 69.0 57.8 47.4 46.4 50.0 51.2 67.3 65.4 68.5 69.6 52.1 50.6 54.5 55.7 27.5 26.3 29.8 31.2 51.5 51.0 54.5 56.1 60.9 59.9 64.4 66.1\n"
     ]
    }
   ],
   "source": [
    "for row in df_yolox.filter(col(\"type\") == \"Table\")[\"text\"]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaecd3a",
   "metadata": {},
   "source": [
    "These indeed seem to correspond to the text in the table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f327b7",
   "metadata": {},
   "source": [
    "## Are the titles accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f24b76cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOX: Exceeding YOLO Series in 2021\n",
      "Zheng Ge∗ Songtao Liu∗† Feng Wang Zeming Li Jian Sun\n",
      "Megvii Technology {gezheng, liusongtao, wangfeng02, lizeming, sunjian}@megvii.com\n",
      "Abstract\n",
      "1. Introduction\n",
      "2. YOLOX\n",
      "2.1. YOLOX-DarkNet53\n",
      "2https://github.com/ultralytics/yolov3 3https://github.com/RangiLyu/nanodet\n",
      "FPN\n",
      "thus train all the following models from scratch.\n",
      "2.2. Other Backbones\n",
      "3. Comparison with the SOTA\n",
      "4. 1st Place on Streaming Perception Challenge (WAD at CVPR 2021)\n",
      "Acknowledge\n",
      "References\n",
      "5. Conclusion\n"
     ]
    }
   ],
   "source": [
    "for row in df_yolox.filter(col(\"type\") == \"Title\")[\"text\"]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45254db8",
   "metadata": {},
   "source": [
    "Mostly good, some places it's wrong but in an understandable way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a40e6",
   "metadata": {},
   "source": [
    "## What kind of metadata do we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e4e0bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_class_prob': 0.9526500105857849,\n",
       " 'coordinates': {'points': [[138.61550903320312, 575.0453377777786],\n",
       "   [138.61550903320312, 1035.1983642578125],\n",
       "   [798.3045654296875, 1035.1983642578125],\n",
       "   [798.3045654296875, 575.0453377777786]],\n",
       "  'system': 'PixelSpace',\n",
       "  'layout_width': 1700,\n",
       "  'layout_height': 2200},\n",
       " 'last_modified': '2024-04-13T14:04:15',\n",
       " 'filetype': 'application/pdf',\n",
       " 'languages': ['eng'],\n",
       " 'page_number': 2,\n",
       " 'filename': '2107.08430.pdf',\n",
       " 'parent_id': None}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yolox.sample(1, seed=42)[\"metadata\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83f9aa80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_class_prob': 0.9438773989677429,\n",
       " 'coordinates': {'points': [[139.2, 1266.9449494444443],\n",
       "   [139.2, 1892.3771716666668],\n",
       "   [800.0421142578125, 1892.3771716666668],\n",
       "   [800.0421142578125, 1266.9449494444443]],\n",
       "  'system': 'PixelSpace',\n",
       "  'layout_width': 1700,\n",
       "  'layout_height': 2200},\n",
       " 'last_modified': '2024-04-13T14:04:15',\n",
       " 'filetype': 'application/pdf',\n",
       " 'languages': ['eng'],\n",
       " 'page_number': 1,\n",
       " 'filename': '2107.08430.pdf',\n",
       " 'parent_id': 'ebf237f98f05c4390df1cde93629de8d'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yolox.filter(col(\"type\")==\"NarrativeText\")[\"metadata\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f058f",
   "metadata": {},
   "source": [
    "Class probabilities are quite interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "80ccca57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_class_prob': None,\n",
       " 'coordinates': {'points': [[169.36388888888888, 1927.1432266666668],\n",
       "   [169.36388888888888, 1978.2544575],\n",
       "   [670.5395, 1978.2544575],\n",
       "   [670.5395, 1927.1432266666668]],\n",
       "  'system': 'PixelSpace',\n",
       "  'layout_width': 1700,\n",
       "  'layout_height': 2200},\n",
       " 'last_modified': '2024-04-13T14:04:15',\n",
       " 'filetype': 'application/pdf',\n",
       " 'languages': ['eng'],\n",
       " 'page_number': 2,\n",
       " 'filename': '2107.08430.pdf',\n",
       " 'parent_id': None}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yolox.filter(col(\"type\") == \"Title\")[7][\"metadata\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b331e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yolox = df_yolox.with_columns(\n",
    "    col(\"metadata\").struct.field(\"detection_class_prob\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d41d59a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 132)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_null_detection_class_probas = df_yolox[\"detection_class_prob\"].is_null().sum()\n",
    "n_null_detection_class_probas, df_yolox.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7050679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "80102c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '# elements null prob = 18; total elements = 132')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(df_yolox[\"detection_class_prob\"], bins=\"auto\", density=True);\n",
    "ax.set_xlabel(\"detection_class_prob\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(f\"# elements null prob = {n_null_detection_class_probas}; total elements = {df_yolox.height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5413ce5d",
   "metadata": {},
   "source": [
    "## Make \"clean\" narrative text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ca8b2bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (11, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>type</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;FigureCaption&quot;</td><td>7</td></tr><tr><td>&quot;Footer&quot;</td><td>7</td></tr><tr><td>&quot;Formula&quot;</td><td>1</td></tr><tr><td>&quot;Header&quot;</td><td>1</td></tr><tr><td>&quot;Image&quot;</td><td>4</td></tr><tr><td>&quot;ListItem&quot;</td><td>33</td></tr><tr><td>&quot;NarrativeText&quot;</td><td>49</td></tr><tr><td>&quot;PageBreak&quot;</td><td>6</td></tr><tr><td>&quot;Table&quot;</td><td>6</td></tr><tr><td>&quot;Title&quot;</td><td>16</td></tr><tr><td>&quot;UncategorizedText&quot;</td><td>2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (11, 2)\n",
       "┌───────────────────┬───────┐\n",
       "│ type              ┆ count │\n",
       "│ ---               ┆ ---   │\n",
       "│ str               ┆ u32   │\n",
       "╞═══════════════════╪═══════╡\n",
       "│ FigureCaption     ┆ 7     │\n",
       "│ Footer            ┆ 7     │\n",
       "│ Formula           ┆ 1     │\n",
       "│ Header            ┆ 1     │\n",
       "│ Image             ┆ 4     │\n",
       "│ ListItem          ┆ 33    │\n",
       "│ NarrativeText     ┆ 49    │\n",
       "│ PageBreak         ┆ 6     │\n",
       "│ Table             ┆ 6     │\n",
       "│ Title             ┆ 16    │\n",
       "│ UncategorizedText ┆ 2     │\n",
       "└───────────────────┴───────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_df(df_yolox[\"type\"].value_counts().sort(\"type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5051db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "retained_types = {\"NarrativeText\", \"PageBreak\", \"Title\"}  # though in general, listitem might include bulletpoints perhaps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "be668f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yolox_clean = df_yolox.filter(\n",
    "    col(\"type\").is_in(retained_types) & col(\"detection_class_prob\").is_not_null()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cbaa065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolox_clean_elements = dict_to_elements(df_yolox_clean.drop('detection_class_prob').to_dicts())\n",
    "yolox_clean_elements_chunked = chunk_by_title(\n",
    "    yolox_clean_elements,\n",
    "    combine_text_under_n_chars=100,\n",
    "    max_characters=3000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f7ace1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yolox_clean = pl.DataFrame(convert_to_dict(yolox_clean_elements_chunked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7320e824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>type</th><th>element_id</th><th>text</th><th>metadata</th></tr><tr><td>str</td><td>str</td><td>str</td><td>struct[5]</td></tr></thead><tbody><tr><td>&quot;CompositeEleme…</td><td>&quot;3119d4a8ae3706…</td><td>&quot;YOLOX: Exceedi…</td><td>{&quot;2107.08430.pdf&quot;,&quot;application/pdf&quot;,[&quot;eng&quot;],&quot;2024-04-13T14:04:15&quot;,1}</td></tr><tr><td>&quot;CompositeEleme…</td><td>&quot;7ff6fba53580cd…</td><td>&quot;1. Introductio…</td><td>{&quot;2107.08430.pdf&quot;,&quot;application/pdf&quot;,[&quot;eng&quot;],&quot;2024-04-13T14:04:15&quot;,1}</td></tr><tr><td>&quot;CompositeEleme…</td><td>&quot;c589f7f19470ac…</td><td>&quot;2. YOLOX\n",
       "\n",
       "2.1.…</td><td>{&quot;2107.08430.pdf&quot;,&quot;application/pdf&quot;,[&quot;eng&quot;],&quot;2024-04-13T14:04:15&quot;,2}</td></tr><tr><td>&quot;CompositeEleme…</td><td>&quot;fc9cb8f9a12df3…</td><td>&quot;Our two analyt…</td><td>{&quot;2107.08430.pdf&quot;,&quot;application/pdf&quot;,[&quot;eng&quot;],&quot;2024-04-13T14:04:15&quot;,2}</td></tr><tr><td>&quot;CompositeEleme…</td><td>&quot;57fd9e5fc0ba13…</td><td>&quot;thus train all…</td><td>{&quot;2107.08430.pdf&quot;,&quot;application/pdf&quot;,[&quot;eng&quot;],&quot;2024-04-13T14:04:15&quot;,3}</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌──────────────────┬──────────────────────────┬──────────────────────────┬─────────────────────────┐\n",
       "│ type             ┆ element_id               ┆ text                     ┆ metadata                │\n",
       "│ ---              ┆ ---                      ┆ ---                      ┆ ---                     │\n",
       "│ str              ┆ str                      ┆ str                      ┆ struct[5]               │\n",
       "╞══════════════════╪══════════════════════════╪══════════════════════════╪═════════════════════════╡\n",
       "│ CompositeElement ┆ 3119d4a8ae37065edda1d5c2 ┆ YOLOX: Exceeding YOLO    ┆ {\"2107.08430.pdf\",\"appl │\n",
       "│                  ┆ a779bd17                 ┆ Series in …              ┆ ication/p…              │\n",
       "│ CompositeElement ┆ 7ff6fba53580cd7f53b1f4d6 ┆ 1. Introduction          ┆ {\"2107.08430.pdf\",\"appl │\n",
       "│                  ┆ ce913d44                 ┆                          ┆ ication/p…              │\n",
       "│                  ┆                          ┆ With the develo…         ┆                         │\n",
       "│ CompositeElement ┆ c589f7f19470ac24a9a7ed1e ┆ 2. YOLOX                 ┆ {\"2107.08430.pdf\",\"appl │\n",
       "│                  ┆ ef5c8183                 ┆                          ┆ ication/p…              │\n",
       "│                  ┆                          ┆ 2.1. YOLOX-DarkNet53     ┆                         │\n",
       "│                  ┆                          ┆                          ┆                         │\n",
       "│                  ┆                          ┆ …                        ┆                         │\n",
       "│ CompositeElement ┆ fc9cb8f9a12df3de61fb2c4a ┆ Our two analytical       ┆ {\"2107.08430.pdf\",\"appl │\n",
       "│                  ┆ 7b4ad1ab                 ┆ experiments i…           ┆ ication/p…              │\n",
       "│ CompositeElement ┆ 57fd9e5fc0ba13f3461c1588 ┆ thus train all the       ┆ {\"2107.08430.pdf\",\"appl │\n",
       "│                  ┆ 88df7110                 ┆ following mod…           ┆ ication/p…              │\n",
       "└──────────────────┴──────────────────────────┴──────────────────────────┴─────────────────────────┘"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yolox_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3e8fb464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YOLOX: Exceeding YOLO Series in 2021\\n\\nZheng Ge∗ Songtao Liu∗† Feng Wang Zeming Li Jian Sun\\n\\nAbstract\\n\\nIn this report, we present some experienced improve- ments to YOLO series, forming a new high-performance detector — YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art re- sults across a large scale range of models: For YOLO- Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in in- dustry, we boost it to 47.3% AP on COCO, outperform- ing the current best practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4- CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and\\n\\nresearchers in practical scenes, and we also provide de- ploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at https://github.com/ Megvii-BaseDetection/YOLOX.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9de5a49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "COMPOSITEELEMENT (3119d4a8ae37065edda1d5c2a779bd17)\n",
      "YOLOX: Exceeding YOLO Series in 2021\n",
      "\n",
      "Zheng Ge∗ Songtao Liu∗† Feng Wang Zeming Li Jian Sun\n",
      "\n",
      "Abstract\n",
      "\n",
      "In this report, we present some experienced improve- ments to YOLO series, forming a new high-performance detector — YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art re- sults across a large scale range of models: For YOLO- Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in in- dustry, we boost it to 47.3% AP on COCO, outperform- ing the current best practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4- CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and\n",
      "\n",
      "researchers in practical scenes, and we also provide de- ploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at https://github.com/ Megvii-BaseDetection/YOLOX.\n",
      "--------------------------------------------\n",
      "COMPOSITEELEMENT (7ff6fba53580cd7f53b1f4d6ce913d44)\n",
      "1. Introduction\n",
      "\n",
      "With the development of object detection, YOLO se- ries [23, 24, 25, 1, 7] always pursuit the optimal speed and accuracy trade-off for real-time applications. They extract the most advanced detection technologies available at the time (e.g., anchors [26] for YOLOv2 [24], Residual Net [9] for YOLOv3 [25]) and optimize the implementation for best practice. Currently, YOLOv5 [7] holds the best trade-off performance with 48.2% AP on COCO at 13.7 ms.1\n",
      "\n",
      "Nevertheless, over the past two years, the major ad- vances in object detection academia have focused on anchor-free detectors [29, 40, 14], advanced label assign- ment strategies [37, 36, 12, 41, 22, 4], and end-to-end (NMS-free) detectors [2, 32, 39]. These have not been inte- grated into YOLO families yet, as YOLOv4 and YOLOv5\n",
      "\n",
      "1we choose the YOLOv5-L model at 640 × 640 resolution and test the model with FP16-precision and batch=1 on a V100 to align the settings of YOLOv4 [1] and YOLOv4-CSP [30] for a fair comparison\n",
      "\n",
      "are still anchor-based detectors with hand-crafted assigning rules for training.\n",
      "\n",
      "That’s what brings us here, delivering those recent ad- vancements to YOLO series with experienced optimiza- tion. Considering YOLOv4 and YOLOv5 may be a little over-optimized for the anchor-based pipeline, we choose YOLOv3 [25] as our start point (we set YOLOv3-SPP as the default YOLOv3). Indeed, YOLOv3 is still one of the most widely used detectors in the industry due to the limited computation resources and the insufﬁcient software support in various practical applications.\n",
      "\n",
      "As shown in Fig. 1, with the experienced updates of the above techniques, we boost the YOLOv3 to 47.3% AP (YOLOX-DarkNet53) on COCO with 640 × 640 res- olution, surpassing the current best practice of YOLOv3 (44.3% AP, ultralytics version2) by a large margin. More- over, when switching to the advanced YOLOv5 architec- ture that adopts an advanced CSPNet [31] backbone and an additional PAN [19] head, YOLOX-L achieves 50.0% AP on COCO with 640 × 640 resolution, outperforming the counterpart YOLOv5-L by 1.8% AP. We also test our de- sign strategies on models of small size. YOLOX-Tiny and YOLOX-Nano (only 0.91M Parameters and 1.08G FLOPs) outperform the corresponding counterparts YOLOv4-Tiny and NanoDet3 by 10% AP and 1.8% AP, respectively.\n",
      "\n",
      "We have released our code at https://github. com/Megvii-BaseDetection/YOLOX, with ONNX, TensorRT, NCNN and Openvino supported. One more thing worth mentioning, we won the 1st Place on Streaming Per- ception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model.\n",
      "--------------------------------------------\n",
      "COMPOSITEELEMENT (c589f7f19470ac24a9a7ed1eef5c8183)\n",
      "2. YOLOX\n",
      "\n",
      "2.1. YOLOX-DarkNet53\n",
      "\n",
      "We choose YOLOv3 [25] with Darknet53 as our base- line. In the following part, we will walk through the whole system designs in YOLOX step by step.\n",
      "\n",
      "Implementation details Our training settings are mostly consistent from the baseline to our ﬁnal model. We train the models for a total of 300 epochs with 5 epochs warm- up on COCO train2017 [17]. We use stochastic gradi- ent descent (SGD) for training. We use a learning rate of lr×BatchSize/64 (linear scaling [8]), with a initial lr = 0.01 and the cosine lr schedule. The weight decay is 0.0005 and the SGD momentum is 0.9. The batch size is 128 by default to typical 8-GPU devices. Other batch sizes in- clude single GPU training also work well. The input size is evenly drawn from 448 to 832 with 32 strides. FPS and\n",
      "\n",
      "latency in this report are all measured with FP16-precision and batch=1 on a single Tesla V100.\n",
      "\n",
      "YOLOv3 baseline Our baseline adopts the architec- ture of DarkNet53 backbone and an SPP layer, referred to YOLOv3-SPP in some papers [1, 7]. We slightly change some training strategies compared to the orig- inal implementation [25], adding EMA weights updat- ing, cosine lr schedule, IoU loss and IoU-aware branch. training cls and obj branch, We use BCE Loss for and IoU Loss for training reg branch. These gen- eral training tricks are orthogonal to the key improve- them on the baseline. ment of YOLOX, we thus put Moreover, we only conduct RandomHorizontalFlip, ColorJitter and multi-scale for data augmentation and discard the RandomResizedCrop strategy, because we found the RandomResizedCrop is kind of overlapped with the planned mosaic augmentation. With those en- hancements, our baseline achieves 38.5% AP on COCO val, as shown in Tab. 2.\n",
      "\n",
      "Decoupled head In object detection, the conﬂict between classiﬁcation and regression tasks is a well-known prob- lem [27, 34]. Thus the decoupled head for classiﬁcation and localization is widely used in the most of one-stage and two-stage detectors [16, 29, 35, 34]. However, as YOLO series’ backbones and feature pyramids ( e.g., FPN [13], PAN [20].) continuously evolving, their detection heads re- main coupled as shown in Fig. 2.\n",
      "--------------------------------------------\n",
      "COMPOSITEELEMENT (fc9cb8f9a12df3de61fb2c4a7b4ad1ab)\n",
      "Our two analytical experiments indicate that the coupled detection head may harm the performance. 1). Replacing YOLO’s head with a decoupled one greatly improves the converging speed as shown in Fig. 3. 2). The decoupled head is essential to the end-to-end version of YOLO (will be described next). One can tell from Tab. 1, the end-to- end property decreases by 4.2% AP with the coupled head, while the decreasing reduces to 0.8% AP for a decoupled head. We thus replace the YOLO detect head with a lite de- coupled head as in Fig. 2. Concretely, it contains a 1 × 1 conv layer to reduce the channel dimension, followed by two parallel branches with two 3 × 3 conv layers respec- tively. We report the inference time with batch=1 on V100 in Tab. 2 and the lite decoupled head brings additional 1.1 ms (11.6 ms v.s. 10.5 ms).\n",
      "\n",
      "Strong data augmentation We add Mosaic and MixUp into our augmentation strategies to boost YOLOX’s per- formance. Mosaic is an efﬁcient augmentation strategy proposed by ultralytics-YOLOv32. It is then widely used in YOLOv4 [1], YOLOv5 [7] and other detectors [3]. MixUp [10] is originally designed for image classiﬁcation task but then modiﬁed in BoF [38] for object detection train- ing. We adopt the MixUp and Mosaic implementation in our model and close it for the last 15 epochs, achieving 42.0% AP in Tab. 2. After using strong data augmentation, we found ImageNet pre-training is no more beneﬁcial, we\n",
      "--------------------------------------------\n",
      "COMPOSITEELEMENT (57fd9e5fc0ba13f3461c158888df7110)\n",
      "thus train all the following models from scratch.\n",
      "\n",
      "Anchor-free Both YOLOv4 [1] and YOLOv5 [7] fol- low the original anchor-based pipeline of YOLOv3 [25]. However, the anchor mechanism has many known prob- lems. First, to achieve optimal detection performance, one needs to conduct clustering analysis to determine a set of optimal anchors before training. Those clustered anchors are domain-speciﬁc and less generalized. Second, anchor mechanism increases the complexity of detection heads, as well as the number of predictions for each image. On some edge AI systems, moving such large amount of predictions between devices (e.g., from NPU to CPU) may become a potential bottleneck in terms of the overall latency.\n",
      "\n",
      "Anchor-free detectors [29, 40, 14] have developed rapidly in the past two year. These works have shown that the performance of anchor-free detectors can be on par with anchor-based detectors. Anchor-free mechanism sig- niﬁcantly reduces the number of design parameters which need heuristic tuning and many tricks involved (e.g., An- chor Clustering [24], Grid Sensitive [11].) for good per- formance, making the detector, especially its training and decoding phase, considerably simpler [29].\n",
      "\n",
      "Switching YOLO to an anchor-free manner is quite sim- ple. We reduce the predictions for each location from 3 to 1 and make them directly predict four values, i.e., two offsets in terms of the left-top corner of the grid, and the height and width of the predicted box. We assign the center lo-\n",
      "\n",
      "cation of each object as the positive sample and pre-deﬁne a scale range, as done in [29], to designate the FPN level for each object. Such modiﬁcation reduces the parameters and GFLOPs of the detector and makes it faster, but obtains better performance – 42.9% AP as shown in Tab. 2.\n",
      "\n",
      "We brieﬂy introduce SimOTA here. SimOTA ﬁrst calcu- lates pair-wise matching degree, represented by cost [4, 5, 12, 2] or quality [33] for each prediction-gt pair. For exam- ple, in SimOTA, the cost between gt gi and prediction pj is calculated as:\n",
      "\n",
      "Multi positives To be consistent with the assigning rule of YOLOv3, the above anchor-free version selects only ONE positive sample (the center location) for each object mean- while ignores other high quality predictions. However, opti- mizing those high quality predictions may also bring beneﬁ- cial gradients, which may alleviates the extreme imbalance of positive/negative sampling during training. We simply assigns the center 3×3 area as positives, also named “center sampling” in FCOS [29]. The performance of the detector improves to 45.0% AP as in Tab. 2, already surpassing the current best practice of ultralytics-YOLOv3 (44.3% AP2).\n",
      "--------------------------------------------\n",
      "COMPOSITEELEMENT (ceb3fcfcf9463e8821a10fced54e02aa)\n",
      "where λ is a balancing coefﬁcient. Lcls ij are class- ﬁciation loss and regression loss between gt gi and predic- tion pj. Then, for gt gi, we select the top k predictions with the least cost within a ﬁxed center region as its positive samples. Finally, the corresponding grids of those positive predictions are assigned as positives, while the rest grids are negatives. Noted that the value k varies for different ground-truth. Please refer to Dynamic k Estimation strat- egy in OTA [4] for more details.\n",
      "\n",
      "SimOTA Advanced label assignment is another important progress of object detection in recent years. Based on our own study OTA [4], we conclude four key insights for an advanced label assignment: 1). loss/quality aware, 2). cen- ter prior, 3). dynamic number of positive anchors4 for each ground-truth (abbreviated as dynamic top-k), 4). global view. OTA meets all four rules above, hence we choose it as a candidate label assigning strategy.\n",
      "\n",
      "Speciﬁcally, OTA [4] analyzes the label assignment from a global perspective and formulate the assigning proce- dure as an Optimal Transport (OT) problem, producing the SOTA performance among the current assigning strate- gies [12, 41, 36, 22, 37]. However, in practice we found solving OT problem via Sinkhorn-Knopp algorithm brings 25% extra training time, which is quite expensive for train- ing 300 epochs. We thus simplify it to dynamic top-k strat- egy, named SimOTA, to get an approximate solution.\n",
      "\n",
      "4The term “anchor” refers to “anchor point” in the context of anchor- free detectors and “grid” in the context of YOLO.\n",
      "\n",
      "SimOTA not only reduces the training time but also avoids additional solver hyperparameters in Sinkhorn- Knopp algorithm. As shown in Tab. 2, SimOTA raises the detector from 45.0% AP to 47.3% AP, higher than the SOTA ultralytics-YOLOv3 by 3.0% AP, showing the power of the advanced assigning strategy.\n",
      "\n",
      "End-to-end YOLO We follow [39] to add two additional conv layers, one-to-one label assignment, and stop gradient. These enable the detector to perform an end-to-end manner, but slightly decreasing the performance and the inference speed, as listed in Tab. 2. We thus leave it as an optional module which is not involved in our ﬁnal models.\n",
      "--------------------------------------------\n",
      "COMPOSITEELEMENT (8c7e9b03ddf7eafc7ac693d8e8b62ef9)\n",
      "2.2. Other Backbones\n",
      "\n",
      "Besides DarkNet53, we also test YOLOX on other back- bones with different sizes, where YOLOX achieves consis- tent improvements against all the corresponding counter- parts.\n",
      "\n",
      "Table 4: Comparison of YOLOX-Tiny and YOLOX-Nano and the counterparts in terms of AP (%) on COCO val. All the models are tested at 416 × 416 resolution.\n",
      "\n",
      "Modiﬁed CSPNet in YOLOv5 To give a fair compar- ison, we adopt the exact YOLOv5’s backbone including modiﬁed CSPNet [31], SiLU activation, and the PAN [19] head. We also follow its scaling rule to product YOLOX- S, YOLOX-M, YOLOX-L, and YOLOX-X models. Com- pared to YOLOv5 in Tab. 3, our models get consistent im- provement by ∼3.0% to ∼1.0% AP, with only marginal time increasing (comes from the decoupled head).\n",
      "\n",
      "Tiny and Nano detectors We further shrink our model as YOLOX-Tiny to compare with YOLOv4-Tiny [30]. For mobile devices, we adopt depth wise convolution to con- struct a YOLOX-Nano model, which has only 0.91M pa- rameters and 1.08G FLOPs. As shown in Tab. 4, YOLOX performs well with even smaller model size than the coun- terparts.\n",
      "\n",
      "Model size and data augmentation In our experiments, all the models keep almost the same learning schedule and optimizing parameters as depicted in 2.1. However, we found that the suitable augmentation strategy varies across different size of models. As Tab. 5 shows, while apply- ing MixUp for YOLOX-L can improve AP by 0.9%, it is better to weaken the augmentation for small models like\n",
      "\n",
      "YOLOX-Nano. Speciﬁcally, we remove the mix up aug- mentation and weaken the mosaic (reduce the scale range from [0.1, 2.0] to [0.5, 1.5]) when training small models, i.e., YOLOX-S, YOLOX-Tiny, and YOLOX-Nano. Such a modiﬁcation improves YOLOX-Nano’s AP from 24.0% to 25.3%.\n",
      "\n",
      "For large models, we also found that stronger augmenta- tion is more helpful. Indeed, our MixUp implementation is part of heavier than the original version in [38]. Inspired by Copypaste [6], we jittered both images by a random sam- pled scale factor before mixing up them. To understand the power of Mixup with scale jittering, we compare it with Copypaste on YOLOX-L. Noted that Copypaste requires extra instance mask annotations while MixUp does not. But as shown in Tab. 5, these two methods achieve competitive performance, indicating that MixUp with scale jittering is a qualiﬁed replacement for Copypaste when no instance mask annotation is available.\n",
      "\n",
      "Table 5: Effects of data augmentation under different model sizes. “Scale Jit.” stands for the range of scale jittering for mosaic image. Instance mask annotations from COCO trainval are used when adopting Copypaste.\n",
      "--------------------------------------------\n",
      "COMPOSITEELEMENT (e30e389496460f430d3a1a057d68f781)\n",
      "3. Comparison with the SOTA\n",
      "\n",
      "There is a tradition to show the SOTA comparing table as in Tab. 6. However, keep in mind that the inference speed of the models in this table is often uncontrolled, as speed varies with software and hardware. We thus use the same hardware and code base for all the YOLO series in Fig. 1, plotting the somewhat controlled speed/accuracy curve.\n",
      "\n",
      "We notice that there are some high performance YOLO series with larger model sizes like Scale-YOLOv4 [30] and YOLOv5-P6 [7]. And the current Transformer based detec- tors [21] push the accuracy-SOTA to ∼60 AP. Due to the time and resource limitation, we did not explore those im- portant features in this report. However, they are already in our scope.\n",
      "--------------------------------------------\n",
      "COMPOSITEELEMENT (f05f3352ea1388a7b5e9e11e81564ebe)\n",
      "4. 1st Place on Streaming Perception Challenge (WAD at CVPR 2021)\n",
      "\n",
      "Streaming Perception Challenge on WAD 2021 is a joint evaluation of accuracy and latency through a recently pro- posed metric: streaming accuracy [15]. The key insight be-\n",
      "\n",
      "hind this metric is to jointly evaluate the output of the entire perception stack at every time instant, forcing the stack to consider the amount of streaming data that should be ig- nored while computation is occurring [15]. We found that the best trade-off point for the metric on 30 FPS data stream is a powerful model with the inference time ≤ 33ms. So we adopt a YOLOX-L model with TensorRT to product our ﬁ- nal model for the challenge to win the 1st place. Please refer to the challenge website5 for more details.\n",
      "--------------------------------------------\n",
      "COMPOSITEELEMENT (5697d1ada311454d453ac7b4406a5e7a)\n",
      "Acknowledge\n",
      "\n",
      "This research was supported by National Key R&D Pro- gram of China (No. 2017YFA0700800). It was also funded by China Postdoctoral Science Foundation (2021M690375) and Beijing Postdoctoral Research Foundation\n",
      "--------------------------------------------\n",
      "COMPOSITEELEMENT (d7310bac72eff6e88cd9773731e276f5)\n",
      "References\n",
      "\n",
      "5. Conclusion\n",
      "\n",
      "In this report, we present some experienced updates to YOLO series, which forms a high-performance anchor- free detector called YOLOX. Equipped with some re- cent advanced detection techniques, i.e., decoupled head, anchor-free, and advanced label assigning strategy, YOLOX achieves a better trade-off between speed and accuracy than other counterparts across all model sizes. It is remarkable that we boost the architecture of YOLOv3, which is still one of the most widely used detectors in industry due to its broad compatibility, to 47.3% AP on COCO, surpassing the current best practice by 3.0% AP. We hope this report can help developers and researchers get better experience in practical scenes.\n",
      "\n",
      "5https://eval.ai/web/challenges/challenge-page/ 800/overview\n",
      "\n",
      "copy-paste is a strong data augmentation method for instance segmentation. In CVPR, 2021. 5\n"
     ]
    }
   ],
   "source": [
    "for row in df_yolox_clean.iter_rows(named=True):\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(f\"{row['type'].upper()} ({row['element_id']})\")\n",
    "    print(f\"{row['text']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb53cb9",
   "metadata": {},
   "source": [
    "Seems pretty clean!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c446fc85",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "11def149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNCATEGORIZEDTEXT(5d05cfc3c8e4a52fd1b3b8bd26648010): 1 2 0 2\n",
      "HEADER(b6a10e1c8fbc287cf2eb289c4a125fb8): g u A 6 ] V C . s c [\n",
      "UNCATEGORIZEDTEXT(b023dc6dfd04cdbde51c00b39bb3e17c): 2 v 0 3 4 8 0 . 7 0 1 2 : v i X r a\n",
      "IMAGE(d723360ef4df68729692648b8fe451de): YOLOX: Exceeding YOLO Zheng Ge* Songtao Liu*t Feng Wang Megvii Technology {gezheng, liusongtao, wangfeng02, wa 2 x aS & 48 O 47 x Sas EA < 45 + Sas at ~@-YOLOX-L M43 x © YOLOv5-L 42 = -- YOLOX-DarkNet53 41 x + -YOLOv5-Darknet53 S 6 %- EfficientDet we & S 8 11 14 #17 20 23 26 29 32 35 38 41 44 V100 batch 1 Latency (ms) 41 35 33 31 29 27 25 2 a1 05\n",
      "TITLE(2e8cad76afca6dc4662f568507657f78): YOLOX: Exceeding YOLO Series in 2021\n",
      "TITLE(a7bf43376383192d1c8a6508d00a105d): Zheng Ge∗ Songtao Liu∗† Feng Wang Zeming Li Jian Sun\n",
      "TITLE(56eb87b8e6b432b9c4ef7f486518229b): Megvii Technology {gezheng, liusongtao, wangfeng02, lizeming, sunjian}@megvii.com\n",
      "IMAGE(09625526fab43bd21f928e71a031a36d): wa 2 x aS & 48 O 47 x Sas EA < 45 + Sas at ~@-YOLOX-L M43 x © YOLOv5-L 42 = -- YOLOX-DarkNet53 41 x + -YOLOv5-Darknet53 S 6 %- EfficientDet we & S 8 11 14 #17 20 23 26 29 32 35 38 41 44 V100 batch 1 Latency (ms) 41 YOLOX-S: @ x EfficientDet-Lite3 35 AX® 33 @ EfficientDet-Lite2 YOLOX-Tiny 31 x 29 EfficientDet-Lite1 27 YOLOX-Nano e 25 EfficientDet-Lited 2 a1 |_NanoDet PPYOLO-Tiny <> YOLOv4-Tiny 05 15 25 35 45 55 65 7.5 85 9.5 10.5 11.5 12.5 Number of parameters (M)\n",
      "FIGURECAPTION(4e7c3846ee93d5ccf8dd273dc96b12c8): Figure 1: Speed-accuracy trade-off of accurate models (top) and Size-accuracy curve of lite models on mobile devices (bottom) for YOLOX and other state-of-the-art object detectors.\n",
      "TITLE(ebf237f98f05c4390df1cde93629de8d): Abstract\n",
      "NARRATIVETEXT(c4dabd97240aae4bbbdacdd5e91b1a9c): In this report, we present some experienced improve- ments to YOLO series, forming a new high-performance detector — YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art re- sults across a large scale range of models: For YOLO- Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in in- dustry, we boost it to 47.3% AP on COCO, outperform- ing the current best practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4- CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and\n",
      "NARRATIVETEXT(901bbffb419c52b83da4a09edd48630a): researchers in practical scenes, and we also provide de- ploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at https://github.com/ Megvii-BaseDetection/YOLOX.\n",
      "TITLE(52d0153ad1bfbd3fa12a62015b8cf66a): 1. Introduction\n",
      "NARRATIVETEXT(d045016abb81d5a9043dc5a1771d1df6): With the development of object detection, YOLO se- ries [23, 24, 25, 1, 7] always pursuit the optimal speed and accuracy trade-off for real-time applications. They extract the most advanced detection technologies available at the time (e.g., anchors [26] for YOLOv2 [24], Residual Net [9] for YOLOv3 [25]) and optimize the implementation for best practice. Currently, YOLOv5 [7] holds the best trade-off performance with 48.2% AP on COCO at 13.7 ms.1\n",
      "NARRATIVETEXT(b561c8648e57bb25d6976f5f9e2e9b33): Nevertheless, over the past two years, the major ad- vances in object detection academia have focused on anchor-free detectors [29, 40, 14], advanced label assign- ment strategies [37, 36, 12, 41, 22, 4], and end-to-end (NMS-free) detectors [2, 32, 39]. These have not been inte- grated into YOLO families yet, as YOLOv4 and YOLOv5\n",
      "LISTITEM(f592443d2bfcb167554527f4373b59d2): Equal contribution. † Corresponding author.\n",
      "NARRATIVETEXT(8413ce5606e709c2653fd1cda5ab8d17): 1we choose the YOLOv5-L model at 640 × 640 resolution and test the model with FP16-precision and batch=1 on a V100 to align the settings of YOLOv4 [1] and YOLOv4-CSP [30] for a fair comparison\n",
      "FOOTER(0fc5165686190ca845407c03ad4572e8): 1\n",
      "PAGEBREAK(e3b0c44298fc1c149afbf4c8996fb924): \n",
      "NARRATIVETEXT(96898c94cbd376b30597d5a902f2224b): are still anchor-based detectors with hand-crafted assigning rules for training.\n",
      "NARRATIVETEXT(500c7c423b55a14619b9e6010d1ab672): That’s what brings us here, delivering those recent ad- vancements to YOLO series with experienced optimiza- tion. Considering YOLOv4 and YOLOv5 may be a little over-optimized for the anchor-based pipeline, we choose YOLOv3 [25] as our start point (we set YOLOv3-SPP as the default YOLOv3). Indeed, YOLOv3 is still one of the most widely used detectors in the industry due to the limited computation resources and the insufﬁcient software support in various practical applications.\n",
      "NARRATIVETEXT(85b5b1f3d4944b7844734eff7ebe1878): As shown in Fig. 1, with the experienced updates of the above techniques, we boost the YOLOv3 to 47.3% AP (YOLOX-DarkNet53) on COCO with 640 × 640 res- olution, surpassing the current best practice of YOLOv3 (44.3% AP, ultralytics version2) by a large margin. More- over, when switching to the advanced YOLOv5 architec- ture that adopts an advanced CSPNet [31] backbone and an additional PAN [19] head, YOLOX-L achieves 50.0% AP on COCO with 640 × 640 resolution, outperforming the counterpart YOLOv5-L by 1.8% AP. We also test our de- sign strategies on models of small size. YOLOX-Tiny and YOLOX-Nano (only 0.91M Parameters and 1.08G FLOPs) outperform the corresponding counterparts YOLOv4-Tiny and NanoDet3 by 10% AP and 1.8% AP, respectively.\n",
      "NARRATIVETEXT(3668ef8fb90744d865d58d03b1e2903d): We have released our code at https://github. com/Megvii-BaseDetection/YOLOX, with ONNX, TensorRT, NCNN and Openvino supported. One more thing worth mentioning, we won the 1st Place on Streaming Per- ception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model.\n",
      "TITLE(4a2f0057b257b441ecd41d50df46b014): 2. YOLOX\n",
      "TITLE(59d72629a253626b6a71936b81c34ef0): 2.1. YOLOX-DarkNet53\n",
      "NARRATIVETEXT(9ba8c498aae6c2f9f4698eeb873735fc): We choose YOLOv3 [25] with Darknet53 as our base- line. In the following part, we will walk through the whole system designs in YOLOX step by step.\n",
      "NARRATIVETEXT(8d8f017d8763243942b0b90afce0b0f1): Implementation details Our training settings are mostly consistent from the baseline to our ﬁnal model. We train the models for a total of 300 epochs with 5 epochs warm- up on COCO train2017 [17]. We use stochastic gradi- ent descent (SGD) for training. We use a learning rate of lr×BatchSize/64 (linear scaling [8]), with a initial lr = 0.01 and the cosine lr schedule. The weight decay is 0.0005 and the SGD momentum is 0.9. The batch size is 128 by default to typical 8-GPU devices. Other batch sizes in- clude single GPU training also work well. The input size is evenly drawn from 448 to 832 with 32 strides. FPS and\n",
      "TITLE(dcec661e7cd8fe4f234212588fd213d8): 2https://github.com/ultralytics/yolov3 3https://github.com/RangiLyu/nanodet\n",
      "FOOTER(5749fdd6b67e4204b3047ba33540bc87): 2\n",
      "TABLE(f4956d38ecee24ddaaf717178faec6ce): Models Coupled Head Decoupled Head Vanilla YOLO End-to-end YOLO 38.5 34.3 (-4.2) 39.6 38.8 (-0.8)\n",
      "FIGURECAPTION(839748ad9941fcc2d6bdaead4b96994c): Table 1: The effect of decoupled head for end-to-end YOLO in terms of AP (%) on COCO.\n",
      "NARRATIVETEXT(cc2325e913e4eea42cfcfaafc435fa60): latency in this report are all measured with FP16-precision and batch=1 on a single Tesla V100.\n",
      "NARRATIVETEXT(f34abb4a286819fe52383d4bdebc156d): YOLOv3 baseline Our baseline adopts the architec- ture of DarkNet53 backbone and an SPP layer, referred to YOLOv3-SPP in some papers [1, 7]. We slightly change some training strategies compared to the orig- inal implementation [25], adding EMA weights updat- ing, cosine lr schedule, IoU loss and IoU-aware branch. training cls and obj branch, We use BCE Loss for and IoU Loss for training reg branch. These gen- eral training tricks are orthogonal to the key improve- them on the baseline. ment of YOLOX, we thus put Moreover, we only conduct RandomHorizontalFlip, ColorJitter and multi-scale for data augmentation and discard the RandomResizedCrop strategy, because we found the RandomResizedCrop is kind of overlapped with the planned mosaic augmentation. With those en- hancements, our baseline achieves 38.5% AP on COCO val, as shown in Tab. 2.\n",
      "NARRATIVETEXT(35bfcbae784724e17502ebf16e2bacd2): Decoupled head In object detection, the conﬂict between classiﬁcation and regression tasks is a well-known prob- lem [27, 34]. Thus the decoupled head for classiﬁcation and localization is widely used in the most of one-stage and two-stage detectors [16, 29, 35, 34]. However, as YOLO series’ backbones and feature pyramids ( e.g., FPN [13], PAN [20].) continuously evolving, their detection heads re- main coupled as shown in Fig. 2.\n",
      "NARRATIVETEXT(566386297ca01d83cd6459b3a7f7c90f): Our two analytical experiments indicate that the coupled detection head may harm the performance. 1). Replacing YOLO’s head with a decoupled one greatly improves the converging speed as shown in Fig. 3. 2). The decoupled head is essential to the end-to-end version of YOLO (will be described next). One can tell from Tab. 1, the end-to- end property decreases by 4.2% AP with the coupled head, while the decreasing reduces to 0.8% AP for a decoupled head. We thus replace the YOLO detect head with a lite de- coupled head as in Fig. 2. Concretely, it contains a 1 × 1 conv layer to reduce the channel dimension, followed by two parallel branches with two 3 × 3 conv layers respec- tively. We report the inference time with batch=1 on V100 in Tab. 2 and the lite decoupled head brings additional 1.1 ms (11.6 ms v.s. 10.5 ms).\n",
      "PAGEBREAK(e3b0c44298fc1c149afbf4c8996fb924): \n",
      "IMAGE(56878f177f71bce3d774683ab8189f4b): Feature 1 YOLOv3~v5 ' 1x1 conv ' Coupled Head #tanchorxC — CIs. I + I I 3x3 conv ' HxXWX J wanchorx4 Reg. I ——__+ — 1 + | 1024 ' #anchorx1 Obj. ' Hxwx | 512 | = 256 7 | YOLOX JA —— > __, Decoupled Head JA I po Ge . _. yt Hxwxc |! ' ce | x2 ' P5 ' HxWx256 1 I a4 ty yy feature (p3 1 JA —s | |Res) axwx4 1 1 L__, I » HxWx256 [Py * 1 l x2 1 I > Hxwx1 ! 1 HxWx256 ' 1\n",
      "TITLE(e3d95833c7a6fd7cd4976917aea3f529): FPN\n",
      "FIGURECAPTION(8ccb1c1ba1ba4647131329648971d1f7): Figure 2: Illustration of the difference between YOLOv3 head and the proposed decoupled head. For each level of FPN feature, we ﬁrst adopt a 1 × 1 conv layer to reduce the feature channel to 256 and then add two parallel branches with two 3 × 3 conv layers each for classiﬁcation and regression tasks respectively. IoU branch is added on the regression branch.\n",
      "IMAGE(fd9de6d384c3d1b9feaa505502830e72): = 03 & 025 Oo 02 o 8 cas o1 —Decoupled head 0.05 —YOLO head 0 50 100 150 200 250 300 Epochs\n",
      "FIGURECAPTION(2fd4b51393ee12ff6791a041650a74bf): Figure 3: Training curves for detectors with YOLOv3 head or decoupled head. We evaluate the AP on COCO val every 10 epochs. It is obvious that the decoupled head converges much faster than the YOLOv3 head and achieves better re- sult ﬁnally.\n",
      "NARRATIVETEXT(830e4b13fc51a0cfaaa943ecf7023918): Strong data augmentation We add Mosaic and MixUp into our augmentation strategies to boost YOLOX’s per- formance. Mosaic is an efﬁcient augmentation strategy proposed by ultralytics-YOLOv32. It is then widely used in YOLOv4 [1], YOLOv5 [7] and other detectors [3]. MixUp [10] is originally designed for image classiﬁcation task but then modiﬁed in BoF [38] for object detection train- ing. We adopt the MixUp and Mosaic implementation in our model and close it for the last 15 epochs, achieving 42.0% AP in Tab. 2. After using strong data augmentation, we found ImageNet pre-training is no more beneﬁcial, we\n",
      "TITLE(36902b60fbdb3b097dbd72c8c49bad3f): thus train all the following models from scratch.\n",
      "NARRATIVETEXT(caba65b897a17443508dd84b2d0e0cc4): Anchor-free Both YOLOv4 [1] and YOLOv5 [7] fol- low the original anchor-based pipeline of YOLOv3 [25]. However, the anchor mechanism has many known prob- lems. First, to achieve optimal detection performance, one needs to conduct clustering analysis to determine a set of optimal anchors before training. Those clustered anchors are domain-speciﬁc and less generalized. Second, anchor mechanism increases the complexity of detection heads, as well as the number of predictions for each image. On some edge AI systems, moving such large amount of predictions between devices (e.g., from NPU to CPU) may become a potential bottleneck in terms of the overall latency.\n",
      "NARRATIVETEXT(001f411bf1f82b438bca7ba8930d9afe): Anchor-free detectors [29, 40, 14] have developed rapidly in the past two year. These works have shown that the performance of anchor-free detectors can be on par with anchor-based detectors. Anchor-free mechanism sig- niﬁcantly reduces the number of design parameters which need heuristic tuning and many tricks involved (e.g., An- chor Clustering [24], Grid Sensitive [11].) for good per- formance, making the detector, especially its training and decoding phase, considerably simpler [29].\n",
      "NARRATIVETEXT(6c879bad7ce4314c57776e2b5bc6347f): Switching YOLO to an anchor-free manner is quite sim- ple. We reduce the predictions for each location from 3 to 1 and make them directly predict four values, i.e., two offsets in terms of the left-top corner of the grid, and the height and width of the predicted box. We assign the center lo-\n",
      "FOOTER(70d25f2c1428def16804c3b346ee8d13): 3\n",
      "PAGEBREAK(e3b0c44298fc1c149afbf4c8996fb924): \n",
      "TABLE(b232f17d844f9659c7a032929de5504a): Methods AP (%) Parameters GFLOPs Latency FPS YOLOv3-ultralytics2 44.3 63.00 M 157.3 10.5 ms 95.2 YOLOv3 baseline +decoupled head +strong augmentation +anchor-free +multi positives +SimOTA +NMS free (optional) 38.5 39.6 (+1.1) 42.0 (+2.4) 42.9 (+0.9) 45.0 (+2.1) 47.3 (+2.3) 46.5 (-0.8) 63.00 M 63.86 M 63.86 M 63.72 M 63.72 M 63.72 M 67.27 M 157.3 186.0 186.0 185.3 185.3 185.3 205.1 10.5 ms 11.6 ms 11.6 ms 11.1 ms 11.1 ms 11.1 ms 13.5 ms 95.2 86.2 86.2 90.1 90.1 90.1 74.1\n",
      "FIGURECAPTION(e73d2b944c6844988f6f93b5d521cb51): Table 2: Roadmap of YOLOX-Darknet53 in terms of AP (%) on COCO val. All the models are tested at 640×640 resolution, with FP16-precision and batch=1 on a Tesla V100. The latency and FPS in this table are measured without post-processing.\n",
      "NARRATIVETEXT(2c8b91744acd7ae7b07ce9d0cd9820fe): cation of each object as the positive sample and pre-deﬁne a scale range, as done in [29], to designate the FPN level for each object. Such modiﬁcation reduces the parameters and GFLOPs of the detector and makes it faster, but obtains better performance – 42.9% AP as shown in Tab. 2.\n",
      "NARRATIVETEXT(d37d14c6d57692663f1d8f3164014391): We brieﬂy introduce SimOTA here. SimOTA ﬁrst calcu- lates pair-wise matching degree, represented by cost [4, 5, 12, 2] or quality [33] for each prediction-gt pair. For exam- ple, in SimOTA, the cost between gt gi and prediction pj is calculated as:\n",
      "NARRATIVETEXT(9178ae44b1cdbce1bb59c95ffc4f68fa): Multi positives To be consistent with the assigning rule of YOLOv3, the above anchor-free version selects only ONE positive sample (the center location) for each object mean- while ignores other high quality predictions. However, opti- mizing those high quality predictions may also bring beneﬁ- cial gradients, which may alleviates the extreme imbalance of positive/negative sampling during training. We simply assigns the center 3×3 area as positives, also named “center sampling” in FCOS [29]. The performance of the detector improves to 45.0% AP as in Tab. 2, already surpassing the current best practice of ultralytics-YOLOv3 (44.3% AP2).\n",
      "FORMULA(2ca1c22727db641d74097acc266d1c3a): ij + λLreg ij , cij =Lcls (1)\n",
      "NARRATIVETEXT(7832143807b44826d9b9baa7f7e1b56d): where λ is a balancing coefﬁcient. Lcls ij are class- ﬁciation loss and regression loss between gt gi and predic- tion pj. Then, for gt gi, we select the top k predictions with the least cost within a ﬁxed center region as its positive samples. Finally, the corresponding grids of those positive predictions are assigned as positives, while the rest grids are negatives. Noted that the value k varies for different ground-truth. Please refer to Dynamic k Estimation strat- egy in OTA [4] for more details.\n",
      "NARRATIVETEXT(6ede7f75e6faa26ac391aaad93d0f5c1): SimOTA Advanced label assignment is another important progress of object detection in recent years. Based on our own study OTA [4], we conclude four key insights for an advanced label assignment: 1). loss/quality aware, 2). cen- ter prior, 3). dynamic number of positive anchors4 for each ground-truth (abbreviated as dynamic top-k), 4). global view. OTA meets all four rules above, hence we choose it as a candidate label assigning strategy.\n",
      "NARRATIVETEXT(e9ecbaf344e0d3bfc68456a2ce44bcde): Speciﬁcally, OTA [4] analyzes the label assignment from a global perspective and formulate the assigning proce- dure as an Optimal Transport (OT) problem, producing the SOTA performance among the current assigning strate- gies [12, 41, 36, 22, 37]. However, in practice we found solving OT problem via Sinkhorn-Knopp algorithm brings 25% extra training time, which is quite expensive for train- ing 300 epochs. We thus simplify it to dynamic top-k strat- egy, named SimOTA, to get an approximate solution.\n",
      "NARRATIVETEXT(2351a19cbc66f3bc55b1a3266f1275a6): 4The term “anchor” refers to “anchor point” in the context of anchor- free detectors and “grid” in the context of YOLO.\n",
      "NARRATIVETEXT(d996c535a5f40b130f8237d9be3688e9): SimOTA not only reduces the training time but also avoids additional solver hyperparameters in Sinkhorn- Knopp algorithm. As shown in Tab. 2, SimOTA raises the detector from 45.0% AP to 47.3% AP, higher than the SOTA ultralytics-YOLOv3 by 3.0% AP, showing the power of the advanced assigning strategy.\n",
      "NARRATIVETEXT(07b0f028ba228080b73318597d27aa02): End-to-end YOLO We follow [39] to add two additional conv layers, one-to-one label assignment, and stop gradient. These enable the detector to perform an end-to-end manner, but slightly decreasing the performance and the inference speed, as listed in Tab. 2. We thus leave it as an optional module which is not involved in our ﬁnal models.\n",
      "TITLE(32fbc04cafc7ee2ff21a9084f1a09776): 2.2. Other Backbones\n",
      "NARRATIVETEXT(f1cb566d9e4a2babaa28fc4e2e9737b6): Besides DarkNet53, we also test YOLOX on other back- bones with different sizes, where YOLOX achieves consis- tent improvements against all the corresponding counter- parts.\n",
      "FOOTER(0b0bd7ca2acebad288fe09c9d9595f1f): 4\n",
      "PAGEBREAK(e3b0c44298fc1c149afbf4c8996fb924): \n",
      "TABLE(ded2017a773ce980c7bc96f4e3cca71a): Models AP (%) Parameters GFLOPs Latency YOLOv5-S YOLOX-S 36.7 39.6 (+2.9) 7.3 M 9.0 M 17.1 26.8 8.7 ms 9.8 ms YOLOv5-M 44.5 YOLOX-M 46.4 (+1.9) 21.4 M 25.3 M 51.4 73.8 11.1 ms 12.3 ms YOLOv5-L YOLOX-L 48.2 50.0 (+1.8) 47.1 M 54.2 M 115.6 155.6 13.7 ms 14.5 ms YOLOv5-X 50.4 YOLOX-X 51.2 (+0.8) 87.8 M 99.1 M 219.0 281.9 16.0 ms 17.3 ms\n",
      "FIGURECAPTION(5e376395259da8d82d75e0f312d29774): Table 3: Comparison of YOLOX and YOLOv5 in terms of AP (%) on COCO. All the models are tested at 640 × 640 resolution, with FP16-precision and batch=1 on a Tesla V100.\n",
      "TABLE(8d88702efd5712ae954515eadbf81a80): Models AP (%) Parameters GFLOPs YOLOv4-Tiny [30] PPYOLO-Tiny YOLOX-Tiny 21.7 22.7 32.8 (+10.1) 6.06 M 4.20 M 5.06 M 6.96 - 6.45 NanoDet3 YOLOX-Nano 23.5 25.3 (+1.8) 0.95 M 0.91 M 1.20 1.08\n",
      "NARRATIVETEXT(be9e01b72bd0affb35de32f3527747a9): Table 4: Comparison of YOLOX-Tiny and YOLOX-Nano and the counterparts in terms of AP (%) on COCO val. All the models are tested at 416 × 416 resolution.\n",
      "NARRATIVETEXT(3136691f026d7d6d7fdaf57f37dc9e6c): Modiﬁed CSPNet in YOLOv5 To give a fair compar- ison, we adopt the exact YOLOv5’s backbone including modiﬁed CSPNet [31], SiLU activation, and the PAN [19] head. We also follow its scaling rule to product YOLOX- S, YOLOX-M, YOLOX-L, and YOLOX-X models. Com- pared to YOLOv5 in Tab. 3, our models get consistent im- provement by ∼3.0% to ∼1.0% AP, with only marginal time increasing (comes from the decoupled head).\n",
      "NARRATIVETEXT(09fe77d847114f3f3ab942770871e6c9): Tiny and Nano detectors We further shrink our model as YOLOX-Tiny to compare with YOLOv4-Tiny [30]. For mobile devices, we adopt depth wise convolution to con- struct a YOLOX-Nano model, which has only 0.91M pa- rameters and 1.08G FLOPs. As shown in Tab. 4, YOLOX performs well with even smaller model size than the coun- terparts.\n",
      "NARRATIVETEXT(af4b68778139ddaae4fa7ef5a1b8d024): Model size and data augmentation In our experiments, all the models keep almost the same learning schedule and optimizing parameters as depicted in 2.1. However, we found that the suitable augmentation strategy varies across different size of models. As Tab. 5 shows, while apply- ing MixUp for YOLOX-L can improve AP by 0.9%, it is better to weaken the augmentation for small models like\n",
      "FOOTER(aec400e3e65dc09b31513694bc9893b9): 5\n",
      "NARRATIVETEXT(15e7fc2ba868f7b91e244063f2e8d119): YOLOX-Nano. Speciﬁcally, we remove the mix up aug- mentation and weaken the mosaic (reduce the scale range from [0.1, 2.0] to [0.5, 1.5]) when training small models, i.e., YOLOX-S, YOLOX-Tiny, and YOLOX-Nano. Such a modiﬁcation improves YOLOX-Nano’s AP from 24.0% to 25.3%.\n",
      "NARRATIVETEXT(1ff462bedaa80fcf4b371c1971653126): For large models, we also found that stronger augmenta- tion is more helpful. Indeed, our MixUp implementation is part of heavier than the original version in [38]. Inspired by Copypaste [6], we jittered both images by a random sam- pled scale factor before mixing up them. To understand the power of Mixup with scale jittering, we compare it with Copypaste on YOLOX-L. Noted that Copypaste requires extra instance mask annotations while MixUp does not. But as shown in Tab. 5, these two methods achieve competitive performance, indicating that MixUp with scale jittering is a qualiﬁed replacement for Copypaste when no instance mask annotation is available.\n",
      "TABLE(4402c55664f03775e2ce50b09d9d2fbb): Models Scale Jit. Extra Aug. AP (%) YOLOX-Nano [0.5, 1.5] [0.1, 2.0] - MixUp 25.3 24.0 (-1.3) YOLOX-L [0.1, 2.0] [0.1, 2.0] - MixUp 48.6 49.5 (+0.9) [0.1, 2.0] Copypaste [6] 49.4\n",
      "NARRATIVETEXT(a1b3ad4c3966024d8772415d98361b29): Table 5: Effects of data augmentation under different model sizes. “Scale Jit.” stands for the range of scale jittering for mosaic image. Instance mask annotations from COCO trainval are used when adopting Copypaste.\n",
      "TITLE(2549cd12226912c6583326fbd32ae9f8): 3. Comparison with the SOTA\n",
      "NARRATIVETEXT(a37069aab2446d940ea447e16c4b18e2): There is a tradition to show the SOTA comparing table as in Tab. 6. However, keep in mind that the inference speed of the models in this table is often uncontrolled, as speed varies with software and hardware. We thus use the same hardware and code base for all the YOLO series in Fig. 1, plotting the somewhat controlled speed/accuracy curve.\n",
      "NARRATIVETEXT(e7052b8577085254fbc29ed321d2bd19): We notice that there are some high performance YOLO series with larger model sizes like Scale-YOLOv4 [30] and YOLOv5-P6 [7]. And the current Transformer based detec- tors [21] push the accuracy-SOTA to ∼60 AP. Due to the time and resource limitation, we did not explore those im- portant features in this report. However, they are already in our scope.\n",
      "TITLE(6b03b1f86caf0a8f125e47d640c23432): 4. 1st Place on Streaming Perception Challenge (WAD at CVPR 2021)\n",
      "NARRATIVETEXT(9abae0039ff1d24bf7124d3be8f185e0): Streaming Perception Challenge on WAD 2021 is a joint evaluation of accuracy and latency through a recently pro- posed metric: streaming accuracy [15]. The key insight be-\n",
      "PAGEBREAK(e3b0c44298fc1c149afbf4c8996fb924): \n",
      "TABLE(520901e70f306ac8b2ca176f61d1b69d): Method Backbone Size FPS AP (%) AP50 AP75 APS APM APL (V100) YOLOv3 + ASFF* [18] Darknet-53 YOLOv3 + ASFF* [18] Darknet-53 608 800 45.5 29.4 42.4 43.9 63.0 64.1 47.4 49.2 25.5 27.0 45.7 46.6 52.3 53.4 EfﬁcientDet-D0 [28] EfﬁcientDet-D1 [28] EfﬁcientDet-D2 [28] EfﬁcientDet-D3 [28] Efﬁcient-B0 Efﬁcient-B1 Efﬁcient-B2 Efﬁcient-B3 512 640 768 896 98.0 74.1 56.5 34.5 33.8 39.6 43.0 45.8 52.2 58.6 62.3 65.0 35.8 42.3 46.2 49.3 12.0 17.9 22.5 26.6 38.3 44.3 47.0 49.4 51.2 56.0 58.4 59.8 PP-YOLOv2 [11] PP-YOLOv2 [11] ResNet50-vd-dcn ResNet101-vd-dcn 640 640 68.9 50.3 49.5 50.3 68.2 69.0 54.4 55.3 30.7 31.6 52.9 53.9 61.2 62.4 YOLOv4 [1] YOLOv4-CSP [30] CSPDarknet-53 Modiﬁed CSP 608 640 62.0 73.0 43.5 47.5 65.7 66.2 47.3 51.7 26.7 28.2 46.7 51.2 53.3 59.8 YOLOv3-ultralytics2 YOLOv5-M [7] YOLOv5-L [7] YOLOv5-X [7] Darknet-53 Modiﬁed CSP v5 Modiﬁed CSP v5 Modiﬁed CSP v5 640 640 640 640 95.2 90.1 73.0 62.5 44.3 44.5 48.2 50.4 64.6 63.1 66.9 68.8 - - - - - - - - - - - - - - - - YOLOX-DarkNet53 YOLOX-M YOLOX-L YOLOX-X Darknet-53 Modiﬁed CSP v5 Modiﬁed CSP v5 Modiﬁed CSP v5 640 640 640 640 90.1 81.3 69.0 57.8 47.4 46.4 50.0 51.2 67.3 65.4 68.5 69.6 52.1 50.6 54.5 55.7 27.5 26.3 29.8 31.2 51.5 51.0 54.5 56.1 60.9 59.9 64.4 66.1\n",
      "FIGURECAPTION(01900234fe27f5f1478c1318c6475177): Table 6: Comparison of the speed and accuracy of different object detectors on COCO 2017 test-dev. We select all the models trained on 300 epochs for fair comparison.\n",
      "NARRATIVETEXT(0bf72c0aa332f80a3041bf43f01d573b): hind this metric is to jointly evaluate the output of the entire perception stack at every time instant, forcing the stack to consider the amount of streaming data that should be ig- nored while computation is occurring [15]. We found that the best trade-off point for the metric on 30 FPS data stream is a powerful model with the inference time ≤ 33ms. So we adopt a YOLOX-L model with TensorRT to product our ﬁ- nal model for the challenge to win the 1st place. Please refer to the challenge website5 for more details.\n",
      "TITLE(b1c6d980951c394fcd4a89e019f418e6): Acknowledge\n",
      "NARRATIVETEXT(140f6ec56151561c602f246dd3906e0e): This research was supported by National Key R&D Pro- gram of China (No. 2017YFA0700800). It was also funded by China Postdoctoral Science Foundation (2021M690375) and Beijing Postdoctoral Research Foundation\n",
      "TITLE(a0d7deccf89e42d02a9d66b0c1889689): References\n",
      "TITLE(6947310b68f318b75c23dd64151edce2): 5. Conclusion\n",
      "LISTITEM(e27c1a4e6cc3dbb79611ca6aea3dc1aa): and Hong- Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020. 1, 2, 3, 6\n",
      "NARRATIVETEXT(a4926edf6c05886df439e4ca0161b7c6): In this report, we present some experienced updates to YOLO series, which forms a high-performance anchor- free detector called YOLOX. Equipped with some re- cent advanced detection techniques, i.e., decoupled head, anchor-free, and advanced label assigning strategy, YOLOX achieves a better trade-off between speed and accuracy than other counterparts across all model sizes. It is remarkable that we boost the architecture of YOLOv3, which is still one of the most widely used detectors in industry due to its broad compatibility, to 47.3% AP on COCO, surpassing the current best practice by 3.0% AP. We hope this report can help developers and researchers get better experience in practical scenes.\n",
      "LISTITEM(dc97f5d9e014cd1cffdcb826114bf2c7): [2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to- end object detection with transformers. In ECCV, 2020. 1, 4\n",
      "LISTITEM(465de5742ab3ebd533ac39c36220c5c1): [3] Qiang Chen, Yingming Wang, Tong Yang, Xiangyu Zhang, Jian Cheng, and Jian Sun. You only look one-level feature. In CVPR, 2021. 3\n",
      "LISTITEM(394bed53f6c520af8124e3ba7570ef08): [4] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian Sun. Ota: Optimal transport assignment for object detection. In CVPR, 2021. 1, 4\n",
      "LISTITEM(c5233069ae1c89565e3e560bc9d0bf38): [5] Zheng Ge, Jianfeng Wang, Xin Huang, Songtao Liu, and Os- amu Yoshie. Lla: Loss-aware label assignment for dense arXiv preprint arXiv:2101.04307, pedestrian detection. 2021. 4\n",
      "NARRATIVETEXT(d38ee8cb0c07c2ff3c0aa94dd4a3965d): 5https://eval.ai/web/challenges/challenge-page/ 800/overview\n",
      "LISTITEM(868b13086f853f4d29a7b759fbb65d1d): [6] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung- Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple\n",
      "FOOTER(29399af043bbf069ecfd1abdcaee4b15): 6\n",
      "PAGEBREAK(e3b0c44298fc1c149afbf4c8996fb924): \n",
      "NARRATIVETEXT(03e540517edd2593feeb5ecb1408ccf5): copy-paste is a strong data augmentation method for instance segmentation. In CVPR, 2021. 5\n",
      "LISTITEM(4a7973dc1f16aa98920b761626a680e2): [7] glenn jocher et al. yolov5. https://github.com/ ultralytics/yolov5, 2021. 1, 2, 3, 5, 6\n",
      "LISTITEM(9b04aaa540203e344a467a20bedb85d1): [8] Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noord- huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, large mini- Yangqing Jia, and Kaiming He. Accurate, arXiv preprint batch sgd: Training imagenet in 1 hour. arXiv:1706.02677, 2017. 2\n",
      "LISTITEM(1dbc0965d38bfc9182952e9313f45c8a): [9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. 2016. 1\n",
      "LISTITEM(31b57c11b1ebd82e8b6a10ef7bcf2463): [10] Zhang Hongyi, Cisse Moustapha, N. Dauphin Yann, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. ICLR, 2018. 3\n",
      "NARRATIVETEXT(c7a0da7189b1a50331f146c8f2047210): [11] Xin Huang, Xinxin Wang, Wenyu Lv, Xiaying Bai, Xiang Long, Kaipeng Deng, Qingqing Dang, Shumin Han, Qiwen Liu, Xiaoguang Hu, et al. Pp-yolov2: A practical object detector. arXiv preprint arXiv:2104.10419, 2021. 3, 6 [12] Kang Kim and Hee Seok Lee. Probabilistic anchor assign- In ECCV,\n",
      "LISTITEM(026246108f2509411e0813a7c34201dc): ment with iou prediction for object detection. 2020. 1, 4\n",
      "NARRATIVETEXT(bfcc91f0cb97b2d421cdd93980aa2015): [13] Seung-Wook Kim, Hyong-Keun Kook, Jee-Young Sun, Mun-Cheon Kang, and Sung-Jea Ko. Parallel feature pyra- mid network for object detection. In ECCV, 2018. 2 [14] Hei Law and Jia Deng. Cornernet: Detecting objects as\n",
      "LISTITEM(293359f83ff24b290a743c0d344beec5): paired keypoints. In ECCV, 2018. 1, 3\n",
      "LISTITEM(2d48e984a6c26ed88b5fd7a740aacf1b): [15] Mengtian Li, Yuxiong Wang, and Deva Ramanan. Towards streaming perception. In ECCV, 2020. 5, 6\n",
      "LISTITEM(064b74b2fb2e571dbf949f1bb09ad750): [16] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense object detection. In ICCV, 2017. 2\n",
      "LISTITEM(4f7335829110b1024f5d804d2a6a60e3): [17] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 2\n",
      "LISTITEM(2be8c2332b84d44e90c49fc169809927): [18] Songtao Liu, Di Huang, and Yunhong Wang. Learning spa- tial fusion for single-shot object detection. arXiv preprint arXiv:1911.09516, 2019. 6\n",
      "LISTITEM(4971988d895cf25ea47b2d3a7be5d106): [19] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. In Path aggregation network for instance segmentation. CVPR, 2018. 2, 5\n",
      "LISTITEM(65c2bcb0a5cad586ab20338308465b8a): [20] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In CVPR, 2018. 2\n",
      "LISTITEM(4b1971c78914e9dc940ae552341a22ab): [21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans- former: Hierarchical vision transformer using shifted win- dows. arXiv preprint arXiv:2103.14030, 2021. 5\n",
      "LISTITEM(47ae2b0224af4aba9c0859c8aecdf34d): [22] Yuchen Ma, Songtao Liu, Zeming Li, and Jian Sun. Iqdet: Instance-wise quality distribution sampling for object detec- tion. In CVPR, 2021. 1, 4\n",
      "LISTITEM(312abc318836cee3d4c0962a4e777cf6): [23] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Uniﬁed, real-time object de- tection. In CVPR, 2016. 1\n",
      "LISTITEM(3253ef52a80d4b58ccb8c1eb9bae3562): [24] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. In CVPR, 2017. 1, 3\n",
      "FOOTER(c69b33366ea0bcfb6c30799a4100c6a0): 7\n",
      "LISTITEM(3bd3c8be12fe7c3e4320546025877895): [25] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018. 1, 2, 3\n",
      "LISTITEM(13f9990de9c6cb169c83beebb0b919fc): [26] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NeurIPS, 2015. 1\n",
      "LISTITEM(14b59241ac468868d3e684506be8073d): [27] Guanglu Song, Yu Liu, and Xiaogang Wang. Revisiting the sibling head in object detector. In CVPR, 2020. 2\n",
      "LISTITEM(7eae1aaad2c44019d2637d0064f98ffd): [28] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efﬁcientdet: Scalable and efﬁcient object detection. In CVPR, 2020. 6\n",
      "LISTITEM(a4af72af505e31a5a221dec70e12df43): [29] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: In ICCV, Fully convolutional one-stage object detection. 2019. 1, 2, 3, 4\n",
      "NARRATIVETEXT(f3068dea4b8ea3523fa2be15e2f34761): and Hong- Yuan Mark Liao. Scaled-yolov4: Scaling cross stage partial network. arXiv preprint arXiv:2011.08036, 2020. 1, 5, 6 [31] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A new backbone that can enhance learning capability of cnn. In CVPR workshops, 2020. 2, 5\n",
      "LISTITEM(20bdebab34580b2ddcbec3d3bae9f7d3): [32] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. End-to-end object detection with fully convolutional network. In CVPR, 2020. 1\n",
      "LISTITEM(4590616653dc1732d33e023dd6c3fffa): [33] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. End-to-end object detection with fully convolutional network. In CVPR, 2021. 4\n",
      "NARRATIVETEXT(090c726e5b77016a1fe1a6413f152c32): [34] Yue Wu, Yinpeng Chen, Lu Yuan, Zicheng Liu, Lijuan Wang, Hongzhi Li, and Yun Fu. Rethinking classiﬁcation and localization for object detection. In CVPR, 2020. 2 [35] Yue Wu, Yinpeng Chen, Lu Yuan, Zicheng Liu, Lijuan Wang, Hongzhi Li, and Yun Fu. Rethinking classiﬁcation and localization for object detection. In CVPR, 2020. 2 [36] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In CVPR, 2020. 1, 4\n",
      "LISTITEM(19c8f658c512403b877686f7b3e463e2): [37] Xiaosong Zhang, Fang Wan, Chang Liu, Rongrong Ji, and Qixiang Ye. Freeanchor: Learning to match anchors for vi- sual object detection. In NeurIPS, 2019. 1, 4\n",
      "LISTITEM(6854c686bfdd3db23d0b8ac7ea097329): [38] Zhi Zhang, Tong He, Hang Zhang, Zhongyuan Zhang, Jun- yuan Xie, and Mu Li. Bag of freebies for training object de- tection neural networks. arXiv preprint arXiv:1902.04103, 2019. 3, 5\n",
      "NARRATIVETEXT(6005974e725d884517157c83826975bc): [39] Qiang Zhou, Chaohui Yu, Chunhua Shen, Zhibin Wang, and Hao Li. Object detection made simpler by eliminating heuristic nms. arXiv preprint arXiv:2101.11782, 2021. 1, 4 [40] Xingyi Zhou, Dequan Wang, and Philipp Kr¨ahenb¨uhl. Ob- jects as points. arXiv preprint arXiv:1904.07850, 2019. 1, 3\n",
      "LISTITEM(8939711e75dd82c34686b9588e02f09d): [41] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong, Songtao Liu, Zeming Li, and Jian Sun. Autoassign: Differ- entiable label assignment for dense object detection. arXiv preprint arXiv:2007.03496, 2020. 1, 4\n"
     ]
    }
   ],
   "source": [
    "for element in out_yolox:\n",
    "    print(f\"{element.category.upper()}({element.id}): {element.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6e2c117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5d05cfc3c8e4a52fd1b3b8bd26648010'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e943a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
